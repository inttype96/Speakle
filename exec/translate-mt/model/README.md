# English-to-Korean Translation Model (LoRA Fine-tuned)

## ğŸ“Œ Overview
ë³¸ ëª¨ë¸ì€ ê³µê°œ ë²ˆì—­ ëª¨ë¸ `NHNDQ/nllb-finetuned-en2ko`ë¥¼ ê¸°ë°˜ìœ¼ë¡œ,  
ì¶”ê°€ì ì¸ **ë„ë©”ì¸ íŠ¹í™” ë°ì´í„°**ì™€ **LoRA (Low-Rank Adaptation)** ê¸°ë²•ì„ ì ìš©í•˜ì—¬  
íŠ¹íˆ **ê°€ì‚¬ ë²ˆì—­ì— ì í•©í•œ ì„±ëŠ¥**ì„ ëª©í‘œë¡œ ê°œë°œëœ ì˜ì–´ â†’ í•œêµ­ì–´(En â†’ Ko) ë²ˆì—­ ëª¨ë¸ì…ë‹ˆë‹¤.  

`NHNDQ/nllb-finetuned-en2ko`ëŠ” Meta AIì˜ `facebook/nllb-200-distilled-600M`ì„  
ê¸°ë°˜ìœ¼ë¡œ ì˜ì–´ â†’ í•œêµ­ì–´ ë²ˆì—­ì— íŠ¹í™”ë˜ë„ë¡ íŒŒì¸ íŠœë‹ëœ ëª¨ë¸ì…ë‹ˆë‹¤.  
í•´ë‹¹ ëª¨ë¸ì€ ë‰´ìŠ¤ ë“± ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ í•™ìŠµí•˜ì—¬  
**ì˜ì–´ í•™ìŠµ ë° ì¼ë°˜ ë¬¸ë§¥ ë²ˆì—­ì— ì í•©í•œ ì„±ëŠ¥**ì„ ë³´ì´ëŠ” ê²ƒìœ¼ë¡œ íŒë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.  

---

## ğŸ“Š Dataset
í•™ìŠµì— ì‚¬ìš©ëœ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:
- **Kaggle Spotify Lyrics Dataset**: ì•½ **40,000ê±´**ì˜ ì˜ì–´ ë…¸ë˜ ê°€ì‚¬ ë°ì´í„°ì— GPT-5 ëª¨ë¸ë¡œ ê³¡ë‹¨ìœ„ë¡œ í•´ì„í•œ í•œêµ­ì–´ ë²ˆì—­ 


---

## âš™ï¸ Training Details

### Fine-tuning Method
- **Base Model**: `facebook/nllb-200-distilled-600M`  
- **Intermediate Model**: `NHNDQ/nllb-finetuned-en2ko`  
- **Fine-tuning Strategy**: **LoRA (Low-Rank Adaptation)**  

### Training Parameters
- **Epochs**: 8 (ì•½ 3,000 step ì²´í¬í¬ì¸íŠ¸ ê²°ê³¼ í™œìš©)  
- **Batch size**: 16 (per-device, Gradient Accumulation 8 â†’ ìœ íš¨ ë°°ì¹˜ 128)  
- **Learning rate**: 1e-4 (LoRA íŒŒë¼ë¯¸í„° í•™ìŠµë¥ , ê¶Œì¥ ë²”ìœ„: 1e-4 ~ 2e-4)  
- **Optimizer**: AdamW (weight decay = 0.01)  
- **Scheduler**: Cosine Annealing (warmup steps = 1,000)  
- **Max input/output length**: 256 tokens (ê°€ì‚¬ 1ì¤„ ê¸°ì¤€ ì¶©ë¶„, ê¸°ì¡´ ê¶Œì¥ê°’ 128 ëŒ€ë¹„ í™•ì¥)  
- **Checkpoint/Evaluation frequency**: 1,000 steps  

---

### LoRA Configuration
- **Rank (r)**: 8  
- **Alpha**: 16  
- **Dropout**: 0.05  
- **Target modules**: `["q_proj", "v_proj"]` (Encoder + Decoder ì „ ì¸µ ì ìš©)  

---

## ğŸ” Decoding Strategy
ëª¨ë¸ ì¶”ë¡  ì‹œ ì ìš©í•œ ë””ì½”ë”© ë°©ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

- **Decoding Method**: Beam Search  
- **Beam size**: 4  
- **Top-k**: ì ìš©í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’)  
- **Top-p (nucleus sampling)**: ì ìš©í•˜ì§€ ì•ŠìŒ (ê¸°ë³¸ê°’)  
- **Temperature**: ì ìš©í•˜ì§€ ì•ŠìŒ (deterministic decoding)  
- **Max input length**: 128 tokens  
- **Max output length**: 128 tokens  
- **Batch size (inference)**: 64  

ì¦‰, ë³¸ ëª¨ë¸ì€ **Beam Search (beam=4)** ê¸°ë°˜ì˜ ê²°ì •ì (decoding without sampling) ë””ì½”ë”© ì „ëµì„ ì‚¬ìš©í•˜ì˜€ìœ¼ë©°,  
Sampling ê¸°ë°˜ ê¸°ë²•(Top-k, Top-p, Temperature)ì€ ì‚¬ìš©í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 

---

## âš ï¸ Limitations
- gpt ì‘ë‹µ ì›ë¬¸ì— ëŒ€í•´ ì¬ê°€ê³µ ìˆ˜í–‰ í•˜ì§€ ì•ŠìŒ. (ì˜ì–´ë¥¼ í•œê¸€ë¡œ ì½ëŠ” í–‰ìœ„, ì˜ì–´ ê·¸ëŒ€ë¡œ í•œêµ­ì–´ ë²ˆì—­ì— ë“¤ì–´ê°„ ë°ì´í„°, ë°˜ë³µ í›„ë ´ ë“± ì œì™¸ ë¯¸ì‹œë„) 
- ë¬¸ë§¥ì ìœ¼ë¡œ ëª¨í˜¸í•œ ì…ë ¥ì— ëŒ€í•´ ë¶€ìì—°ìŠ¤ëŸ¬ìš´ ë²ˆì—­ ê°€ëŠ¥ì„± ì¡´ì¬  

---

