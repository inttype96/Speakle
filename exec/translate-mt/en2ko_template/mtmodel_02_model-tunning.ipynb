{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4c1c809-d71d-47ab-9c19-e989b1ebc854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PY: /home/j-j13c104/.conda/envs/c104-env/bin/python\n",
      "VER: 3.9.23\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "train_lora_en2ko.py\n",
    "- NHNDQ/nllb-finetuned-en2ko 에 LoRA 적용하여 en→ko 파인튜닝\n",
    "- 48GB VRAM 환경에서 약 85% 사용 목표(자동 배치 크기 탐색)\n",
    "- 중간 저장(체크포인트) 및 재개(resume) 지원\n",
    "\n",
    "필수 패키지:\n",
    "  pip install -U \"torch>=2.6\" transformers datasets peft accelerate sacrebleu sentencepiece\n",
    "\n",
    "데이터 가정:\n",
    "  /popsongData/parsed_en2ko_train.csv\n",
    "  /popsongData/parsed_en2ko_valid.csv\n",
    "  /popsongData/parsed_en2ko_test.csv\n",
    "  컬럼명: english(입력), korean_ref(정답)  ← 다르면 SRC_COL/TGT_COL 변경\n",
    "\"\"\"\n",
    "\n",
    "import os, math, time, gc, torch, pandas as pd\n",
    "# os.environ[\"TRANSFORMERS_NO_ACCELERATE\"] = \"1\"\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  \n",
    "\n",
    "\n",
    "# torch.set_num_interop_threads(2)  # ← 병렬 작업 시작 전, 최초 1회\n",
    "# torch.set_num_threads(6)\n",
    "\n",
    "import sys, platform\n",
    "print(\"PY:\", sys.executable)\n",
    "print(\"VER:\", platform.python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0b556cc8-4223-485f-b8cb-b773a6022e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import ( AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainer, Seq2SeqTrainingArguments, EarlyStoppingCallback )\n",
    "from peft import LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "832719de-fcbb-426a-82b4-fa00d495a636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 0) 경로/데이터 컬럼 설정\n",
    "# =========================\n",
    "DATA_DIR  = \"./popsongData\"\n",
    "# TRAIN_CSV = f\"{DATA_DIR}/parsed_en2ko_train.csv\"\n",
    "TRAIN_CSV = f\"{DATA_DIR}/build_dataset_enko_more_train.csv\"\n",
    "\n",
    "# VALID_CSV = f\"{DATA_DIR}/parsed_en2ko_valid.csv\"\n",
    "# TEST_CSV  = f\"{DATA_DIR}/parsed_en2ko_test.csv\"\n",
    "VALID_CSV = f\"{DATA_DIR}/build_dataset_enko_more_valid.csv\"\n",
    "TEST_CSV  = f\"{DATA_DIR}/build_dataset_enko_more_test.csv\"\n",
    "\n",
    "SRC_COL = \"en\"     # 입력 문장 컬럼명\n",
    "TGT_COL = \"ko\"  # 정답(한국어) 컬럼명  (다르면 여기만 바꾸면 됨)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "827197ca-3003-4bd6-95f7-dc921fe0b145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1) 모델/출력/학습 하이퍼파라미터\n",
    "# =========================\n",
    "MODEL_ID = \"NHNDQ/nllb-finetuned-en2ko\"\n",
    "OUT_DIR  = \"./outputs/en2ko-nllb600m-lora-other-parms4\"  # 체크포인트/최종 모델 저장 디렉토리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "db779d93-20a1-44e4-8603-d6b24c9047a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 입력/출력 최대 길이(토큰 단위). 가사 1줄 번역 기준 128이면 보통 충분\n",
    "# MAX_SRC, MAX_TGT = 112, 112\n",
    "MAX_SRC, MAX_TGT = 256, 256\n",
    "# 학습률/에폭 등 기본 학습 설정\n",
    "# LR            = 2e-4\n",
    "LR            = 1e-4            # LoRA 파라미터 학습률(1e-4~2e-4 권장)\n",
    "EPOCHS        = 8               # 데이터 많으면 1, 적으면 2 (과적합 주의)\n",
    "WARMUP_STEPS  = 1000            # 초반 학습 안정화\n",
    "WEIGHT_DECAY  = 0.01            # AdamW 가중치 감쇠(과적합 방지)\n",
    "SAVE_STEPS    = 1000            # 이 스텝마다 체크포인트 저장\n",
    "EVAL_STEPS    = 1000            # 이 스텝마다 검증 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "345eeb6d-33ab-471b-b4e1-2ba1189f1b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA 설정: q_proj, v_proj만(가성비). 필요 시 k_proj/out_proj 추가 가능\n",
    "LORA_R        = 8               # 랭크(표현력↔메모리 트레이드오프)\n",
    "LORA_ALPHA    = 16              # 스케일(일반적으로 r와 비슷하거나 2배)\n",
    "LORA_DROPOUT  = 0.05            # LoRA 브랜치 드롭아웃\n",
    "TARGET_MODULES = [\"q_proj\",\"v_proj\"]  # Encoder+Decoder 모든 층에 자동 매칭"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44babc23-39ad-4599-9630-4c9d0b51de2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2) VRAM 사용 목표 및 배치 자동탐색 설정\n",
    "# =========================\n",
    "TARGET_VRAM_FRAC = 0.85   # 사용 목표(48GB 기준 ~40.8GB 정도)\n",
    "MARGIN_FRAC      = 0.03   # 학습 중 실제 사용량 증가 대비 여유 마진\n",
    "START_BSZ        = 16     # per-device 배치 탐색 시작값\n",
    "MAX_BSZ_CEIL     = 256    # per-device 배치 상한(안전용)\n",
    "GRAD_ACC         = 8      # 그래디언트 누적(유효 배치 = per_device_bsz * GRAD_ACC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fba196fe-25ad-4cbf-8855-245501f2343c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ====== 디바이스/정밀도 ======\n",
    "device = \"cuda\"\n",
    "use_bf16 = torch.cuda.is_available() and torch.cuda.is_bf16_supported()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d79d16-b3a0-412a-8a53-d8fbb0e96924",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ---------- 1) 토크나이저/모델 ----------\n",
    "tok = AutoTokenizer.from_pretrained(MODEL_ID,local_files_only=True)\n",
    "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_ID, use_safetensors=True,local_files_only=True )\n",
    "tok.src_lang = \"eng_Latn\"\n",
    "KOR_ID = tok.convert_tokens_to_ids(\"kor_Hang\")\n",
    "base.config.forced_bos_token_id = KOR_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68f83248-bd56-484a-8468-e571495315b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForSeq2SeqLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): M2M100ForConditionalGeneration(\n",
       "      (model): M2M100Model(\n",
       "        (shared): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "        (encoder): M2M100Encoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x M2M100EncoderLayer(\n",
       "              (self_attn): M2M100Attention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (activation_fn): ReLU()\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): M2M100Decoder(\n",
       "          (embed_tokens): M2M100ScaledWordEmbedding(256206, 1024, padding_idx=1)\n",
       "          (embed_positions): M2M100SinusoidalPositionalEmbedding()\n",
       "          (layers): ModuleList(\n",
       "            (0-11): 12 x M2M100DecoderLayer(\n",
       "              (self_attn): M2M100Attention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (activation_fn): ReLU()\n",
       "              (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (encoder_attn): M2M100Attention(\n",
       "                (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                (v_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (q_proj): lora.Linear(\n",
       "                  (base_layer): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (lora_dropout): ModuleDict(\n",
       "                    (default): Dropout(p=0.05, inplace=False)\n",
       "                  )\n",
       "                  (lora_A): ModuleDict(\n",
       "                    (default): Linear(in_features=1024, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (lora_B): ModuleDict(\n",
       "                    (default): Linear(in_features=8, out_features=1024, bias=False)\n",
       "                  )\n",
       "                  (lora_embedding_A): ParameterDict()\n",
       "                  (lora_embedding_B): ParameterDict()\n",
       "                  (lora_magnitude_vector): ModuleDict()\n",
       "                )\n",
       "                (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              )\n",
       "              (encoder_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "              (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "              (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (lm_head): Linear(in_features=1024, out_features=256206, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LoRA 부착\n",
    "peft_cfg = LoraConfig(\n",
    "    r=LORA_R, lora_alpha=LORA_ALPHA, lora_dropout=LORA_DROPOUT,\n",
    "    task_type=\"SEQ_2_SEQ_LM\", target_modules=TARGET_MODULES, bias=\"none\"\n",
    ")\n",
    "model = get_peft_model(base, peft_cfg).to(device)\n",
    "\n",
    "model.enable_input_require_grads()   # 입력에 grad 요청(ckpt 사용할 때 필수)\n",
    "model.config.use_cache = False       # ckpt와 충돌 방지\n",
    "model.train()                        # 학습 모드 확실히"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "be4eddb4-1dc6-4a6e-a1ef-29fb81ad1da9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "440881e76f6b4e1989d1edc9b1cc7952",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ca36671fb2b4d79a8eebad167459606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042d279d3889417096706b9586a737df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ---------- 2) 데이터 ----------\n",
    "# \n",
    "files = {\"train\": TRAIN_CSV, \"validation\": VALID_CSV, \"test\": TEST_CSV}\n",
    "raw = load_dataset(\"csv\", data_files=files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f102d3e-dfe3-4af9-aaf2-4684bb79e4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(batch):\n",
    "    inputs  = tok(batch[SRC_COL],\n",
    "                  max_length=MAX_SRC,\n",
    "                  truncation=True)\n",
    "\n",
    "    targets = tok(text_target=batch[TGT_COL], \n",
    "                  max_length=MAX_TGT,\n",
    "                  truncation=True)\n",
    "\n",
    "    \n",
    "    inputs[\"labels\"] = targets[\"input_ids\"]   \n",
    "\n",
    "    return inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "92364c8f-a52e-4670-917d-2c28c2718b00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5023d877e43c4f31bea1d40672995c8d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/36195 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e2d296284a24c6b9007a8c6f893ca0d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3217 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d086880a21f4330be89f48e8138040b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/805 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"   # 전처리 때만\n",
    "tokd = raw.map(preprocess, batched=True, remove_columns=raw[\"train\"].column_names)\n",
    "collator = DataCollatorForSeq2Seq(tokenizer=tok, model=model, padding=\"longest\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5a050aed-8690-450f-ab57-4ce4dfbc9b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 3) VRAM 오토튜닝 (per-device batch 탐색) ----------\n",
    "def current_vram_frac():\n",
    "    free, total = torch.cuda.mem_get_info()\n",
    "    used = total - free\n",
    "    return used / total\n",
    "\n",
    "def fits_with_batch(bsz_try: int) -> bool:\n",
    "    try:\n",
    "        # 작은 더미 배치로 forward+backward 한번 수행\n",
    "        dummy = tok([\"hello\"] * bsz_try, return_tensors=\"pt\", padding=True,\n",
    "                    truncation=True, max_length=MAX_SRC).to(device)\n",
    "        labels = tok([\"안녕\"] * bsz_try, return_tensors=\"pt\", padding=True,\n",
    "                     truncation=True, max_length=MAX_TGT)[\"input_ids\"].to(device)\n",
    "        dummy[\"labels\"] = labels\n",
    "        out = model(**dummy)\n",
    "        (out.loss / GRAD_ACC).backward()\n",
    "        model.zero_grad(set_to_none=True)\n",
    "        torch.cuda.synchronize()\n",
    "        del dummy, labels, out\n",
    "        gc.collect(); torch.cuda.empty_cache()\n",
    "        return True\n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            torch.cuda.empty_cache()\n",
    "            return False\n",
    "        raise\n",
    "\n",
    "def autotune_bsz(start=START_BSZ, ceiling=MAX_BSZ_CEIL, target_frac=TARGET_VRAM_FRAC):\n",
    "    # 증가시키며 맞춰보기 → OOM 나면 반으로 낮추는 식의 이분 탐색\n",
    "    lo, hi = 1, start\n",
    "    while hi <= ceiling and fits_with_batch(hi) and current_vram_frac() <= target_frac:\n",
    "        lo = hi\n",
    "        hi *= 2\n",
    "\n",
    "    # 이분 탐색\n",
    "    left, right = lo, min(hi, ceiling)\n",
    "    best = left\n",
    "    while left <= right:\n",
    "        mid = (left + right) // 2\n",
    "        ok = fits_with_batch(mid) and current_vram_frac() <= target_frac\n",
    "        if ok:\n",
    "            best = mid\n",
    "            left = mid + 1\n",
    "        else:\n",
    "            right = mid - 1\n",
    "    return max(1, best)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "27f705e8-32f4-4405-b3f2-66ff3af07987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AutoTune] per_device_train_batch_size = 6 (grad_acc=8, eff_bsz=48)\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # 학습 돌릴 때는 안전하게\n",
    "PER_DEV_BSZ = 6\n",
    "print(f\"[AutoTune] per_device_train_batch_size = {PER_DEV_BSZ} (grad_acc={GRAD_ACC}, eff_bsz={PER_DEV_BSZ*GRAD_ACC})\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a30719e8-de81-4e43-9ded-c175c20dd7eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformers: 4.56.2\n",
      "torch: 2.5.1+cu121\n",
      "transformers.training_args_seq2seq\n",
      "Seq2SeqTrainingArguments\n",
      "/home/j-j13c104/.conda/envs/c104-env/lib/python3.9/site-packages/transformers/__init__.py\n"
     ]
    }
   ],
   "source": [
    "import transformers, torch\n",
    "print(\"transformers:\", transformers.__version__)\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(Seq2SeqTrainingArguments.__module__)\n",
    "print(Seq2SeqTrainingArguments.__name__)\n",
    "import transformers\n",
    "print(transformers.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "37198359-7e03-4677-8aba-41425b2979d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- 4) 트레이너/세이브 재개 ----------\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=OUT_DIR,\n",
    "    num_train_epochs=EPOCHS,\n",
    "    learning_rate=LR,\n",
    "    lr_scheduler_type=\"linear\",\n",
    "    warmup_steps=WARMUP_STEPS,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "\n",
    "    per_device_train_batch_size=PER_DEV_BSZ,\n",
    "    per_device_eval_batch_size=min(PER_DEV_BSZ, 64),\n",
    "    gradient_accumulation_steps=GRAD_ACC,\n",
    "\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=EVAL_STEPS,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=SAVE_STEPS,\n",
    "    save_total_limit=5,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "\n",
    "    predict_with_generate=False,\n",
    "    generation_max_length=MAX_TGT,\n",
    "    bf16=use_bf16,\n",
    "    fp16=not use_bf16,\n",
    "    dataloader_num_workers=2,\n",
    "    logging_steps=100,\n",
    "    report_to=\"none\",\n",
    "\n",
    "    save_safetensors=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4658f4f1-77ac-4b84-bca8-717658801cf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2738239/1564291146.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Seq2SeqTrainer(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokd[\"train\"],\n",
    "    eval_dataset=tokd[\"validation\"],\n",
    "    tokenizer=tok,\n",
    "    data_collator=collator,\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8db81774-b768-491a-870a-950abdf54891",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6040' max='6040' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6040/6040 44:50, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>1.299000</td>\n",
       "      <td>1.215894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>1.221200</td>\n",
       "      <td>1.158332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.184800</td>\n",
       "      <td>1.135449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.171800</td>\n",
       "      <td>1.123840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>1.166900</td>\n",
       "      <td>1.118778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>1.145700</td>\n",
       "      <td>1.116290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== DONE ==\n"
     ]
    }
   ],
   "source": [
    "# 재개 지원: 중간에 끊겨도 마지막 checkpoint에서 이어감\n",
    "last_ckpt = None\n",
    "if os.path.isdir(OUT_DIR):\n",
    "    cks = [os.path.join(OUT_DIR, d) for d in os.listdir(OUT_DIR) if d.startswith(\"checkpoint-\")]\n",
    "    if cks:\n",
    "        last_ckpt = sorted(cks, key=lambda p: int(p.split(\"-\")[-1]))[-1]\n",
    "        print(f\"[Resume] from {last_ckpt}\")\n",
    "\n",
    "trainer.train(resume_from_checkpoint=last_ckpt)\n",
    "trainer.save_model(OUT_DIR)\n",
    "tok.save_pretrained(OUT_DIR)\n",
    "\n",
    "print(\"== DONE ==\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa7bf4e-65e6-47d0-8830-821e894df365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (c104-env)",
   "language": "python",
   "name": "c104-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
